<?php

namespace Emilia\EcoFriendlyRobotsTxt;

/**
 * Handles the eco-friendly robots.txt modifications.
 */
class Plugin {

	const BACKUP_PATH = ABSPATH . 'robots.txt.eco-friendly-backup';

	/**
	 * Initialize the hooks and filters.
	 */
	public function init() {
		add_filter( 'robots_txt', [ $this, 'modify_robots_txt' ], 10, 2 );
		register_activation_hook( __FILE__, [ $this, 'backup_static_robots_txt' ] );
		register_deactivation_hook( __FILE__, [ $this, 'restore_static_robots_txt' ] );
	}

	/**
	 * Backs up the static robots.txt file if it exists.
	 */
	public function backup_static_robots_txt() {
		$robots_path = ABSPATH . 'robots.txt';
		if ( file_exists( $robots_path ) ) {
			// phpcs:ignore Generic.Commenting.Todo.TaskFound
			// @todo Ask for user consent before proceeding.
			// This step will need to be handled via admin notice or a settings page.
			// For the sake of this example, we'll assume consent is given.

			$wp_filesystem = $this->get_filesystem();
			$wp_filesystem::move( $robots_path, self::BACKUP_PATH );
		}
	}

	/**
	 * Restores the static robots.txt file upon plugin deactivation.
	 *
	 * @uses WP_Filesystem
	 */
	public function restore_static_robots_txt() {
		if ( file_exists( self::BACKUP_PATH ) ) {
			$robots_path = ABSPATH . 'robots.txt';

			$wp_filesystem = $this->get_filesystem();
			$wp_filesystem::move( self::BACKUP_PATH, $robots_path );
		}
	}

	/**
	 * Modifies the content of the dynamic robots.txt generated by WordPress.
	 *
	 * @param string $output      The original robots.txt content.
	 * @param bool   $site_public Whether the site is public.
	 *
	 * @return string Modified robots.txt content.
	 */
	public function modify_robots_txt( $output, $site_public ) {
		if ( ! $site_public ) {
			return "User-agent: *\nDisallow: /\n";
		}

		$robots_txt = "# This site is very specific about who it allows crawling from. Our default is you're not allowed to crawl:\n";
		$robots_txt .= "User-agent: *\n";
		$robots_txt .= "Disallow: /\n";

		$allowlist = [
			'Googlebot', // Google.
			'AdsBot-Google', // Google Ads.
			'MediaPartners-Google', // Google AdSense.
			'Applebot', // Apple.
			'Yandex', // Yandex.
			'Baiduspider', // Baidu.
			'Bingbot',   // Bing.
			'Slurp',     // Yahoo.
			'DuckDuckBot', // DuckDuckGo.
			'ia_archiver', // Archive.org.
			'FacebookExternalHit', // Facebook.
			'Twitterbot', // X.
			'LinkedInBot', // LinkedIn.
		];
		$robots_txt .= "# Crawlers that are allowed to crawl this site:\n";
		foreach ( $allowlist as $crawler ) {
			$robots_txt .= "User-agent: $crawler\n";
		}

		$blocked_paths = [
			'/wp-json/',
			'/?rest_route=',
			'/wp-admin/',
			'/wp-content/cache/',
			'/wp-content/plugins/',
			'/xmlrpc.php',
			'/wp-includes/',
		];
		$robots_txt .= "# Paths that are blocked even for allowed crawlers:\n";
		foreach( $blocked_paths as $path ) {
			$robots_txt .= "Disallow: $path\n";
		}

		$allowed_paths = [
			'/wp-includes/css/',
			'/wp-includes/js/',
		];
		$robots_txt .= "# Paths within the blocked paths that are allowed:\n";
		foreach( $allowed_paths as $path ) {
			$robots_txt .= "Allow: $path\n";
		}

		// Extra new line for readability.
		$robots_txt .= "\n";

		// Keep existing Sitemap references.
		if ( strpos( $output, 'Sitemap: ' ) !== false ) {
			preg_match_all( '/Sitemap: (.+)/', $output, $matches );
			foreach ( $matches[0] as $sitemap ) {
				$robots_txt .= "# XML Sitemap:\n";
				$robots_txt .= "$sitemap\n";
			}
		}

		return $robots_txt;
	}

	/**
	 * Retrieves the WordPress filesystem.
	 *
	 * @return WP_Filesystem
	 */
	private function get_filesystem() {
		global $wp_filesystem;

		require_once ABSPATH . '/wp-admin/includes/file.php';
		WP_Filesystem();
		return $wp_filesystem;
	}
}
